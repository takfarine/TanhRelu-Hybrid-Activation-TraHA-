{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass traha(nn.Module):\n    \"\"\"\n    TanhRelu Hybrid Activation (TraHA) Function.\n\n    This activation function is a combination of the hyperbolic tangent (tanh) \n    and the Rectified Linear Unit (ReLU). The intention behind this hybrid \n    function is to leverage the advantages of both tanh (outputs being zero-centered \n    and bounded in the range [-1, 1]) and ReLU (non-saturation for positive inputs).\n\n    The formula is defined as:\n    f(x) = alpha * tanh(x) + beta * relu(x)\n\n    where:\n    - alpha: Hyperparameter to control the contribution of the tanh component.\n    - beta: Hyperparameter to control the contribution of the ReLU component.\n\n    Args:\n    - alpha (float, optional): Scaling factor for the tanh component. Default: 1.0.\n    - beta (float, optional): Scaling factor for the ReLU component. Default: 1.0.\n\n    Examples:\n    >>> activation = TraHA(alpha=1.0, beta=1.0)\n    >>> input_tensor = torch.tensor([-1.0, -0.5, 0.0, 0.5, 1.0])\n    >>> output_tensor = activation(input_tensor)\n    >>> print(output_tensor)\n    \"\"\"\n    \n    def __init__(self, alpha=1.0, beta=1.0):\n        super(traha, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n        \n    def forward(self, x):\n        tanh_component = self.alpha * torch.tanh(x)\n        relu_component = self.beta * torch.relu(x)\n        return tanh_component + relu_component\n\n    \n# Define a simple model\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.traha = traha(alpha=1.0, beta=1.0)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.traha(self.fc1(x))\n        x = nn.Softmax(dim=1)(self.fc2(x))\n        return x\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-22T18:20:39.405766Z","iopub.execute_input":"2023-08-22T18:20:39.406134Z","iopub.status.idle":"2023-08-22T18:20:42.681372Z","shell.execute_reply.started":"2023-08-22T18:20:39.406092Z","shell.execute_reply":"2023-08-22T18:20:42.680014Z"},"trusted":true},"execution_count":1,"outputs":[]}]}