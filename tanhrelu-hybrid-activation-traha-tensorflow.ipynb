{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef traha(x, alpha=1.0, beta=1.0):\n    \"\"\"\n    TanhRelu Hybrid Activation (TraHA) Function in TensorFlow.\n\n    This activation function is a fusion of the hyperbolic tangent (tanh) \n    and the Rectified Linear Unit (ReLU). The primary motivation behind this \n    hybrid function is to amalgamate the benefits of both tanh (with its \n    zero-centered and bounded outputs in the range [-1, 1]) and ReLU \n    (exhibiting non-saturation for positive inputs).\n\n    Mathematically, TraHA is represented as:\n    f(x) = alpha * tanh(x) + beta * relu(x)\n\n    Parameters:\n    - x (tensor): Input tensor.\n    - alpha (float, optional): Hyperparameter that modulates the contribution of the tanh component. Default: 1.0.\n    - beta (float, optional): Hyperparameter that modulates the contribution of the ReLU component. Default: 1.0.\n\n    Returns:\n    - tensor: Output tensor after applying TraHA.\n\n    Example:\n    >>> input_tensor = tf.constant([-1.0, -0.5, 0.0, 0.5, 1.0])\n    >>> output_tensor = traha(input_tensor, alpha=1.0, beta=1.0)\n    >>> print(output_tensor)\n    \"\"\"\n    \n    tanh_component = alpha * tf.math.tanh(x)\n    relu_component = beta * tf.nn.relu(x)\n    return tanh_component + relu_component\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}