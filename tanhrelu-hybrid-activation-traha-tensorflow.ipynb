{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n\ndef traha(x, alpha=1.0, beta=1.0):\n    \"\"\"\n    TanhRelu Hybrid Activation (TraHA) Function.\n    \n    Args:\n    - x (tensor): Input tensor.\n    - alpha (float): Hyperparameter to control the contribution of the tanh component.\n    - beta (float): Hyperparameter to control the contribution of the ReLU component.\n\n    Returns:\n    - tensor: Output after applying TraHA.\n    \"\"\"\n    \n    tanh_component = alpha * tf.math.tanh(x)\n    relu_component = beta * tf.nn.relu(x)\n    \n    return tanh_component + relu_component\n\nget_custom_objects().update({'traha': Activation(traha)})\n\n# Define a simple model\nmodel = Sequential([\n    Dense(128, input_shape=(784,), activation='traha'),\n    Dense(10, activation='softmax')\n])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}